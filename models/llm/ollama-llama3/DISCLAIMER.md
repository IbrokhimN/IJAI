# Disclaimer

**Important:** This project relies on the local LLaMA3 model (`llama3:latest`) accessed via Ollama. 

- **Prerequisite:** Ollama must be pre-installed on your system. The project **does not automatically install Ollama**.  
- **Model availability:** The `llama3:latest` model must be present locally. If the model is missing, the Python wrapper may attempt to run `installer.sh`.  

**Note for users:**  
- If you encounter errors when running the Python code, it is recommended to **manually execute `installer.sh`** to install the required model.  
- The installer script is provided in the repository for convenience, but running it manually ensures that all dependencies and models are properly set up.  

By using this project, you acknowledge that you are responsible for having the proper environment and dependencies installed before running the code.
