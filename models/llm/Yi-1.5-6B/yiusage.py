from llama_cpp import Llama
model_path = "Yi-1.5-6B-Chat-Q4_K_M.gguf"

# –∑–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
llm = Llama(
    model_path=model_path,
    n_ctx=4096,   # –º–æ–∂–Ω–æ 2048 –µ—Å–ª–∏ –º–∞–ª–æ RAM
    n_threads=8,  # –ø–æ–¥—Å—Ç—Ä–æ–π –ø–æ–¥ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —è–¥–µ—Ä
    verbose=False
)

print("ü§ñ Yi-1.5 Chat (–≤–≤–µ–¥–∏ 'exit' —á—Ç–æ–±—ã –≤—ã–π—Ç–∏)\n")

messages = [{"role": "system", "content": "You are a helpful assistant."}]

while True:
    user_input = input("üë§ You: ")
    if user_input.lower() in {"exit", "quit"}:
        break

    messages.append({"role": "user", "content": user_input})
    print("ü§ñ Bot: ", end="", flush=True)

    # —Å—Ç—Ä–∏–º–∏–º –æ—Ç–≤–µ—Ç –ø–æ —á–∞—Å—Ç—è–º
    stream = llm.create_chat_completion(
        messages=messages,
        max_tokens=200,
        temperature=0.7,
        stream=True
    )

    reply = ""
    for chunk in stream:
        delta = chunk["choices"][0]["delta"]
        if "content" in delta:
            text = delta["content"]
            reply += text
            print(text, end="", flush=True)

    print("\n")
    messages.append({"role": "assistant", "content": reply})
